{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5cb0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python dlib face_recognition numpy pillow\n",
    "#!pip install dlib face_recognition opencv-python pillow\n",
    "#!pip install -q ultralytics\n",
    "#!pip install opencv-python\n",
    "#!pip install numpy\n",
    "#!pip install mediapipe\n",
    "#!pip install --upgrade mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d20ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "臉部辨識模型載入成功，包含 69 個人物\n",
      "YOLO 姿態檢測模型已成功載入\n",
      "MediaPipe 手部檢測模型已成功載入\n",
      "\n",
      "==== 綜合人體姿態、手勢和臉部辨識系統 ====\n",
      "1. 添加新人物到臉部辨識系統\n",
      "2. 處理單張圖片\n",
      "3. 處理影片\n",
      "4. 從攝影機即時辨識\n",
      "0. 退出\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "class ComprehensiveRecognitionSystem:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化系統\"\"\"\n",
    "        self._init_directories()\n",
    "        self._init_face_models()\n",
    "        self._init_pose_model()\n",
    "        self._init_hand_model()\n",
    "        \n",
    "    def _init_directories(self):\n",
    "        \"\"\"創建必要的目錄\"\"\"\n",
    "        self.train_dir = \"training_data\"\n",
    "        self.model_dir = \"models\"\n",
    "        self.temp_dir = \"temp\"\n",
    "        \n",
    "        for directory in [self.train_dir, self.model_dir, self.temp_dir]:\n",
    "            Path(directory).mkdir(exist_ok=True)\n",
    "    \n",
    "    def _init_face_models(self):\n",
    "        \"\"\"初始化臉部辨識模型\"\"\"\n",
    "        # 初始化 dlib 模型\n",
    "        self.face_detector = dlib.get_frontal_face_detector()\n",
    "        \n",
    "        # 檢查 shape_predictor 模型是否存在\n",
    "        shape_predictor_path = r\"C:\\Users\\User\\project\\shape_predictor_5_face_landmarks.dat\"\n",
    "        if not os.path.exists(shape_predictor_path):\n",
    "            print(f\"警告：找不到 {shape_predictor_path}，部分臉部對齊功能將不可用\")\n",
    "            self.shape_predictor = None\n",
    "        else:\n",
    "            self.shape_predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "            \n",
    "        # 檢查臉部識別模型是否存在\n",
    "        face_rec_model_path = r\"C:\\Users\\User\\project\\dlib_face_recognition_resnet_model_v1.dat\"\n",
    "        if not os.path.exists(face_rec_model_path):\n",
    "            print(f\"警告：找不到 {face_rec_model_path}，部分臉部識別功能將不可用\")\n",
    "            self.face_recognizer = None\n",
    "        else:\n",
    "            self.face_recognizer = dlib.face_recognition_model_v1(face_rec_model_path)\n",
    "        \n",
    "        # OpenCV 的人臉檢測器作為備用\n",
    "        self.haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
    "        \n",
    "        # 載入已知人臉編碼\n",
    "        self.known_face_encodings = []\n",
    "        self.known_face_names = []\n",
    "        self.load_face_model()\n",
    "    \n",
    "    def _init_pose_model(self):\n",
    "        \"\"\"初始化 YOLO 姿態檢測模型\"\"\"\n",
    "        try:\n",
    "            self.pose_model = YOLO(\"yolo11n-pose.pt\")\n",
    "            print(\"YOLO 姿態檢測模型已成功載入\")\n",
    "        except Exception as e:\n",
    "            print(f\"警告：無法載入 YOLO 姿態檢測模型：{e}\")\n",
    "            self.pose_model = None\n",
    "    \n",
    "    def _init_hand_model(self):\n",
    "        \"\"\"初始化 MediaPipe 手勢檢測模型\"\"\"\n",
    "        try:\n",
    "            # 檢查 MediaPipe 手部檢測模型是否存在\n",
    "            model_path = r\"C:\\Users\\User\\project\\hand_landmarker.task\"\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"警告：找不到 {model_path}，手勢辨識功能將不可用\")\n",
    "                self.hand_detector = None\n",
    "            else:\n",
    "                base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "                options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=2)\n",
    "                self.hand_detector = vision.HandLandmarker.create_from_options(options)\n",
    "                print(\"MediaPipe 手部檢測模型已成功載入\")\n",
    "        except Exception as e:\n",
    "            print(f\"警告：無法初始化 MediaPipe 手部檢測模型：{e}\")\n",
    "            self.hand_detector = None\n",
    "        \n",
    "        # 用於跟蹤手腕位置的字典\n",
    "        self.prev_wrist_x = {}\n",
    "    \n",
    "    # ================ 臉部辨識功能 ================\n",
    "    \n",
    "    def load_face_model(self):\n",
    "        \"\"\"載入先前訓練過的臉部辨識模型\"\"\"\n",
    "        model_path = os.path.join(self.model_dir, \"face_encodings.pickle\")\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "                self.known_face_encodings = data[\"encodings\"]\n",
    "                self.known_face_names = data[\"names\"]\n",
    "            print(f\"臉部辨識模型載入成功，包含 {len(self.known_face_names)} 個人物\")\n",
    "        else:\n",
    "            print(\"沒有找到現有臉部辨識模型，需要先訓練\")\n",
    "    \n",
    "    def add_person(self, person_name, images_folder):\n",
    "        \"\"\"添加新人臉到辨識系統\"\"\"\n",
    "        if not os.path.exists(images_folder):\n",
    "            print(f\"錯誤：找不到資料夾 {images_folder}\")\n",
    "            return False\n",
    "        \n",
    "        image_paths = [os.path.join(images_folder, f) for f in os.listdir(images_folder) \n",
    "                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        if not image_paths:\n",
    "            print(f\"錯誤：資料夾 {images_folder} 中沒有圖片檔案\")\n",
    "            return False\n",
    "        \n",
    "        face_encodings = []\n",
    "        successful_images = 0\n",
    "        \n",
    "        print(f\"開始處理 {person_name} 的 {len(image_paths)} 張圖片...\")\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                # 使用 PIL 載入圖片（更穩定）\n",
    "                pil_image = Image.open(img_path)\n",
    "                # 轉換為 RGB（移除 alpha 通道如果有的話）\n",
    "                if pil_image.mode != 'RGB':\n",
    "                    pil_image = pil_image.convert('RGB')\n",
    "                # 轉換為 numpy 數組給 face_recognition 使用\n",
    "                img = np.array(pil_image)\n",
    "                \n",
    "                # 檢測人臉 - 使用 face_recognition 庫（基於 dlib）\n",
    "                face_locations = face_recognition.face_locations(img, model=\"hog\")\n",
    "                \n",
    "                # 如果找不到人臉，嘗試使用 OpenCV 的正面和側面檢測器\n",
    "                if not face_locations:\n",
    "                    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                    # 先檢測正面\n",
    "                    faces = self.haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "                    if len(faces) == 0:\n",
    "                        # 再檢測側面\n",
    "                        faces = self.profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "                    \n",
    "                    if len(faces) > 0:\n",
    "                        # 轉換 OpenCV 坐標到 face_recognition 格式 (top, right, bottom, left)\n",
    "                        x, y, w, h = faces[0]\n",
    "                        face_locations = [(y, x+w, y+h, x)]\n",
    "                \n",
    "                # 如果還是找不到人臉\n",
    "                if not face_locations:\n",
    "                    print(f\"警告：無法在 {img_path} 中檢測到人臉\")\n",
    "                    continue\n",
    "                \n",
    "                # 計算人臉特徵編碼\n",
    "                encodings = face_recognition.face_encodings(img, face_locations)\n",
    "                \n",
    "                if encodings:\n",
    "                    face_encodings.extend(encodings)\n",
    "                    successful_images += 1\n",
    "                    print(f\"成功處理: {img_path}\")\n",
    "                else:\n",
    "                    print(f\"警告：無法在 {img_path} 中創建有效的人臉編碼\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"處理 {img_path} 時發生錯誤: {e}\")\n",
    "        \n",
    "        print(f\"成功處理 {successful_images}/{len(image_paths)} 張圖片\")\n",
    "        \n",
    "        # 如果有成功處理的圖片，將他們加入到模型中\n",
    "        if face_encodings:\n",
    "            self.known_face_encodings.extend(face_encodings)\n",
    "            self.known_face_names.extend([person_name] * len(face_encodings))\n",
    "            \n",
    "            # 儲存模型\n",
    "            self.save_face_model()\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"錯誤：無法為 {person_name} 創建任何有效的人臉編碼\")\n",
    "            return False\n",
    "    \n",
    "    def save_face_model(self):\n",
    "        \"\"\"保存臉部辨識模型到檔案\"\"\"\n",
    "        data = {\n",
    "            \"encodings\": self.known_face_encodings,\n",
    "            \"names\": self.known_face_names\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.model_dir, \"face_encodings.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        \n",
    "        print(f\"臉部辨識模型已保存，包含 {len(self.known_face_names)} 個人臉\")\n",
    "    \n",
    "    # ================ 手勢識別功能 ================\n",
    "    \n",
    "    def is_ok_sign(self, landmarks):\n",
    "        \"\"\"檢測是否為 OK 手勢\"\"\"\n",
    "        thumb_tip = np.array([landmarks[4].x, landmarks[4].y])\n",
    "        index_tip = np.array([landmarks[8].x, landmarks[8].y])\n",
    "        distance = np.linalg.norm(thumb_tip - index_tip)\n",
    "        middle_tip = landmarks[12].y\n",
    "        ring_tip = landmarks[16].y\n",
    "        pinky_tip = landmarks[20].y\n",
    "        palm_base = landmarks[0].y\n",
    "        return distance < 0.05 and middle_tip < palm_base and ring_tip < palm_base and pinky_tip < palm_base\n",
    "    \n",
    "    def is_peace_sign(self, landmarks):\n",
    "        \"\"\"檢測是否為 V 手勢\"\"\"\n",
    "        def is_extended(pip, tip):\n",
    "            return tip.y < pip.y\n",
    "        return (\n",
    "            is_extended(landmarks[6], landmarks[8]) and\n",
    "            is_extended(landmarks[10], landmarks[12]) and\n",
    "            landmarks[16].y > landmarks[14].y and\n",
    "            landmarks[20].y > landmarks[18].y\n",
    "        )\n",
    "    \n",
    "    def is_open_palm(self, landmarks):\n",
    "        \"\"\"檢測是否為張開手掌\"\"\"\n",
    "        return all(landmarks[tip].y < landmarks[tip - 2].y for tip in [8, 12, 16, 20])\n",
    "    \n",
    "    def is_fist(self, landmarks):\n",
    "        \"\"\"檢測是否為握拳\"\"\"\n",
    "        wrist = np.array([landmarks[0].x, landmarks[0].y])\n",
    "        closed_fingers = [np.linalg.norm(wrist - np.array([landmarks[i].x, landmarks[i].y])) < 0.1 for i in [8, 12, 16, 20]]\n",
    "        return all(closed_fingers)\n",
    "    \n",
    "    def is_clapping(self, hands):\n",
    "        \"\"\"檢測是否為拍手\"\"\"\n",
    "        if len(hands) != 2:\n",
    "            return False\n",
    "        p1 = np.array([hands[0][5].x, hands[0][5].y])\n",
    "        p2 = np.array([hands[1][5].x, hands[1][5].y])\n",
    "        return np.linalg.norm(p1 - p2) < 0.08\n",
    "    \n",
    "    def is_raising_hand(self, wrist, shoulder):\n",
    "        \"\"\"檢測是否為舉手\"\"\"\n",
    "        if np.all(wrist == 0) or np.all(shoulder == 0):\n",
    "            return False\n",
    "        \n",
    "        y_diff = shoulder[1] - wrist[1]\n",
    "        x_diff = abs(wrist[0] - shoulder[0])\n",
    "        return y_diff > 20 and x_diff < 200\n",
    "    \n",
    "    def is_squatting(self, hip, knee):\n",
    "        \"\"\"檢測是否為蹲下\"\"\"\n",
    "        if np.all(hip == 0) or np.all(knee == 0):\n",
    "            return False\n",
    "        return (knee[1] - hip[1]) < 40\n",
    "    \n",
    "    def is_waving_hand(self, track_id, wrist):\n",
    "        \"\"\"檢測是否為揮手\"\"\"\n",
    "        if np.all(wrist == 0):\n",
    "            return False\n",
    "        x_now = wrist[0]\n",
    "        waving = False\n",
    "        if track_id in self.prev_wrist_x:\n",
    "            diff = abs(x_now - self.prev_wrist_x[track_id])\n",
    "            if diff > 20:\n",
    "                waving = True\n",
    "        self.prev_wrist_x[track_id] = x_now\n",
    "        return waving\n",
    "    \n",
    "    # ================ 畫面處理功能 ================\n",
    "    \n",
    "    def draw_hand_landmarks(self, image, detection_result, thickness=3):\n",
    "        \"\"\"繪製手部標記點和手勢\"\"\"\n",
    "        annotated_image = image.copy()\n",
    "        hands = detection_result.hand_landmarks if detection_result else []\n",
    "        clap_shown = self.is_clapping(hands) if hands else False\n",
    "\n",
    "        for landmarks in hands or []:\n",
    "            gesture = \"\"\n",
    "            if self.is_ok_sign(landmarks):\n",
    "                gesture = \"OK Sign\"\n",
    "            elif self.is_peace_sign(landmarks):\n",
    "                gesture = \"Peace Sign\"\n",
    "            elif self.is_open_palm(landmarks):\n",
    "                gesture = \"Open Palm\"\n",
    "            elif self.is_fist(landmarks):\n",
    "                gesture = \"Fist\"\n",
    "            elif clap_shown:\n",
    "                gesture = \"Clap\"\n",
    "\n",
    "            for landmark in landmarks:\n",
    "                x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "                cv2.circle(annotated_image, (x, y), 7, (0, 0, 255), -1)\n",
    "            \n",
    "            connections = mp.solutions.hands.HAND_CONNECTIONS\n",
    "            for connection in connections:\n",
    "                start_idx, end_idx = connection\n",
    "                start = landmarks[start_idx]\n",
    "                end = landmarks[end_idx]\n",
    "                start_point = (int(start.x * image.shape[1]), int(start.y * image.shape[0]))\n",
    "                end_point = (int(end.x * image.shape[1]), int(end.y * image.shape[0]))\n",
    "                cv2.line(annotated_image, start_point, end_point, (0, 255, 0), thickness)\n",
    "\n",
    "            if gesture and not clap_shown:\n",
    "                x, y = int(landmarks[0].x * image.shape[1]), int(landmarks[0].y * image.shape[0])\n",
    "                cv2.putText(annotated_image, gesture, (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
    "\n",
    "        if clap_shown:\n",
    "            cv2.putText(annotated_image, \"Clap\", (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 3)\n",
    "\n",
    "        return annotated_image\n",
    "    \n",
    "    # ================ 主要運行功能 ================\n",
    "    \n",
    "    def recognize_face(self, image, tolerance=0.6):\n",
    "        \"\"\"在圖像中識別人臉\"\"\"\n",
    "        if not self.known_face_encodings:\n",
    "            print(\"錯誤：臉部辨識模型尚未訓練，請先添加人物\")\n",
    "            return image\n",
    "        \n",
    "        # 檢測所有人臉\n",
    "        face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "        \n",
    "        # 如果找不到人臉，使用 OpenCV 嘗試檢測正面和側面\n",
    "        if not face_locations:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            # 檢測正面\n",
    "            frontal_faces = self.haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "            for (x, y, w, h) in frontal_faces:\n",
    "                face_locations.append((y, x+w, y+h, x))\n",
    "        \n",
    "            # 檢測側面\n",
    "            profile_faces = self.profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "            for (x, y, w, h) in profile_faces:\n",
    "                # 檢查是否與已檢測的面孔重疊\n",
    "                is_new = True\n",
    "                for (top, right, bottom, left) in face_locations:\n",
    "                    # 如果中心點落在已有的人臉框內，則不添加\n",
    "                    center_x, center_y = x + w//2, y + h//2\n",
    "                    if left <= center_x <= right and top <= center_y <= bottom:\n",
    "                        is_new = False\n",
    "                        break\n",
    "                if is_new:\n",
    "                    face_locations.append((y, x+w, y+h, x))\n",
    "        \n",
    "        # 如果有檢測到人臉，進行識別\n",
    "        if face_locations:\n",
    "            face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "            \n",
    "            # 辨識每個人臉\n",
    "            for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "                # 與所有已知人臉比對\n",
    "                matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding, tolerance=tolerance)\n",
    "                name = \"未知\"\n",
    "                \n",
    "                # 如果有匹配項，使用距離最近的那個\n",
    "                if True in matches:\n",
    "                    face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "                    best_match_index = np.argmin(face_distances)\n",
    "                    confidence = 1 - face_distances[best_match_index]\n",
    "                    if matches[best_match_index]:\n",
    "                        name = f\"{self.known_face_names[best_match_index]} ({confidence:.2f})\"\n",
    "                \n",
    "                # 繪製人臉框和名字\n",
    "                cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                cv2.rectangle(image, (left, bottom - 35), (right, bottom), (0, 255, 0), cv2.FILLED)\n",
    "                cv2.putText(image, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"處理單一幀，應用所有識別功能\"\"\"\n",
    "        original_frame = frame.copy()\n",
    "        results = None\n",
    "        \n",
    "        # 1. 使用 YOLO 進行姿態識別\n",
    "        if self.pose_model:\n",
    "            try:\n",
    "                results = self.pose_model.track(frame, persist=True, tracker=\"botsort.yaml\")\n",
    "                \n",
    "                if results:\n",
    "                    for r in results:\n",
    "                        annotated = r.plot()\n",
    "                        \n",
    "                        if hasattr(r, \"keypoints\") and r.keypoints is not None:\n",
    "                            kps = r.keypoints.xy.numpy()\n",
    "                            if len(kps) > 0:\n",
    "                                for idx, kp in enumerate(kps):\n",
    "                                    track_id = int(r.id[idx]) if hasattr(r, \"id\") and r.id is not None else idx\n",
    "                                    \n",
    "                                    # 提取關鍵點\n",
    "                                    lw = kp[9]; rw = kp[10]\n",
    "                                    lsh = kp[5]; rsh = kp[6]\n",
    "                                    lhip = kp[11]; rhip = kp[12]\n",
    "                                    lknee = kp[13]; rknee = kp[14]\n",
    "                                    \n",
    "                                    # 檢測姿態\n",
    "                                    left_up = self.is_raising_hand(lw, lsh)\n",
    "                                    right_up = self.is_raising_hand(rw, rsh)\n",
    "                                    squat_left = self.is_squatting(lhip, lknee)\n",
    "                                    squat_right = self.is_squatting(rhip, rknee)\n",
    "                                    wave_left = self.is_waving_hand(f\"{track_id}_L\", lw)\n",
    "                                    wave_right = self.is_waving_hand(f\"{track_id}_R\", rw)\n",
    "                                    \n",
    "                                    # 顯示結果\n",
    "                                    text = \"\"\n",
    "                                    if left_up and right_up:\n",
    "                                        text = \"both hands \"\n",
    "                                    elif left_up:\n",
    "                                        text = \"left hand \"\n",
    "                                    elif right_up:\n",
    "                                        text = \"right hand \"\n",
    "                                    \n",
    "                                    if squat_left and squat_right:\n",
    "                                        text += \"knees down \"\n",
    "                                    if wave_left or wave_right:\n",
    "                                        text += \"waving \"\n",
    "                                    \n",
    "                                    if text:\n",
    "                                        x, y = int(kp[0][0]), int(kp[0][1])\n",
    "                                        cv2.putText(annotated, f\"ID {track_id} {text}\", \n",
    "                                                   (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                        \n",
    "                        frame = annotated.copy()\n",
    "            except Exception as e:\n",
    "                print(f\"YOLO 姿態識別錯誤: {e}\")\n",
    "        \n",
    "        # 2. 使用 MediaPipe 進行手勢識別\n",
    "        if self.hand_detector:\n",
    "            try:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "                detection_result = self.hand_detector.detect(mp_image)\n",
    "                \n",
    "                if detection_result:\n",
    "                    frame = self.draw_hand_landmarks(frame, detection_result)\n",
    "            except Exception as e:\n",
    "                print(f\"MediaPipe 手勢識別錯誤: {e}\")\n",
    "        \n",
    "        # 3. 使用 face_recognition 進行臉部辨識\n",
    "        try:\n",
    "            frame = self.recognize_face(frame)\n",
    "        except Exception as e:\n",
    "            print(f\"臉部辨識錯誤: {e}\")\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"處理單張圖片\"\"\"\n",
    "        try:\n",
    "            # 載入圖片\n",
    "            pil_image = Image.open(image_path)\n",
    "            if pil_image.mode != 'RGB':\n",
    "                pil_image = pil_image.convert('RGB')\n",
    "            \n",
    "            # 轉換為 numpy 數組\n",
    "            image = np.array(pil_image)\n",
    "            \n",
    "            # 處理圖片\n",
    "            result_img = self.process_frame(image)\n",
    "            \n",
    "            # 儲存結果\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            result_path = os.path.join(self.temp_dir, f\"result_{timestamp}.jpg\")\n",
    "            \n",
    "            # 將 numpy 數組轉回 PIL 圖像\n",
    "            result_pil = Image.fromarray(result_img)\n",
    "            \n",
    "            # 保存圖片\n",
    "            result_pil.save(result_path, quality=95)\n",
    "            print(f\"結果已保存至 {result_path}\")\n",
    "            \n",
    "            return result_path\n",
    "        except Exception as e:\n",
    "            print(f\"處理圖片時發生錯誤: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_video(self, video_path, output_path=None, process_every_n_frames=1):\n",
    "        \"\"\"處理影片\"\"\"\n",
    "        # 開啟影片\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(f\"錯誤：無法開啟影片檔案 {video_path}\")\n",
    "            return None\n",
    "        \n",
    "        # 獲取影片屬性\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        print(f\"影片資訊：\")\n",
    "        print(f\"- 解析度：{width}x{height}\")\n",
    "        print(f\"- FPS：{fps}\")\n",
    "        print(f\"- 總幀數：{total_frames}\")\n",
    "        print(f\"- 時長：{total_frames/fps:.2f} 秒\")\n",
    "        \n",
    "        # 如果需要儲存輸出影片\n",
    "        out = None\n",
    "        if output_path:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        # 處理影片\n",
    "        frame_count = 0\n",
    "        recognition_stats = {}\n",
    "        \n",
    "        print(\"開始處理影片...\")\n",
    "        \n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # 顯示進度\n",
    "                if frame_count % 30 == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"處理進度：{progress:.1f}% ({frame_count}/{total_frames} 幀)\")\n",
    "                \n",
    "                # 只處理每第 n 幀以提高效能\n",
    "                if frame_count % process_every_n_frames == 0:\n",
    "                    # 處理幀\n",
    "                    processed_frame = self.process_frame(frame)\n",
    "                    \n",
    "                    # 在畫面左上角顯示幀數和時間\n",
    "                    time_text = f\"幀：{frame_count} | 時間：{frame_count/fps:.2f}s\"\n",
    "                    cv2.putText(processed_frame, time_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                    \n",
    "                    # 如果需要儲存輸出影片\n",
    "                    if out:\n",
    "                        out.write(processed_frame)\n",
    "                    \n",
    "                    # 顯示結果\n",
    "                    cv2.imshow('影片處理', cv2.resize(processed_frame, (900, 680)))\n",
    "                    if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "                        print(\"使用者中斷處理\")\n",
    "                        break\n",
    "                \n",
    "                frame_count += 1\n",
    "                \n",
    "        finally:\n",
    "            # 釋放資源\n",
    "            cap.release()\n",
    "            if out:\n",
    "                out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        print(f\"\\n影片處理完成！\")\n",
    "        if output_path:\n",
    "            print(f\"輸出影片已儲存至：{output_path}\")\n",
    "        \n",
    "        return recognition_stats\n",
    "    \n",
    "    def process_webcam(self, camera_id=0):\n",
    "        \"\"\"從網路攝影機進行即時識別\"\"\"\n",
    "        # 開啟攝影機\n",
    "        cap = cv2.VideoCapture(camera_id)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"錯誤：無法開啟攝影機\")\n",
    "            return\n",
    "        \n",
    "        print(\"開始即時辨識。按空白鍵結束。\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # 讀取一幀\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    print(\"無法從攝影機獲取畫面\")\n",
    "                    break\n",
    "                \n",
    "                # 處理幀\n",
    "                processed_frame = self.process_frame(frame)\n",
    "                \n",
    "                # 顯示結果\n",
    "                cv2.imshow('即時辨識', processed_frame)\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "                    break\n",
    "                \n",
    "        finally:\n",
    "            # 釋放資源\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化系統\n",
    "    system = ComprehensiveRecognitionSystem()\n",
    "    \n",
    "    # 顯示功能選單\n",
    "    print(\"\\n==== 綜合人體姿態、手勢和臉部辨識系統 ====\")\n",
    "    print(\"1. 添加新人物到臉部辨識系統\")\n",
    "    print(\"2. 處理單張圖片\")\n",
    "    print(\"3. 處理影片\")\n",
    "    print(\"4. 從攝影機即時辨識\")\n",
    "    print(\"0. 退出\")\n",
    "    \n",
    "    choice = input(\"\\n請選擇功能 (0-4): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        person_name = input(\"請輸入人物名稱: \")\n",
    "        images_folder = input(\"請輸入包含該人物照片的資料夾路徑: \")\n",
    "        system.add_person(person_name, images_folder)\n",
    "        \n",
    "    elif choice == \"2\":\n",
    "        image_path = input(\"請輸入圖片路徑: \")\n",
    "        system.process_image(image_path)\n",
    "        \n",
    "    elif choice == \"3\":\n",
    "        video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e76fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "臉部辨識模型載入成功，包含 69 個人物\n",
      "YOLO 姿態檢測模型已成功載入\n",
      "MediaPipe 手勢檢測器已成功載入\n",
      "\n",
      "==== 影片人臉和姿態辨識系統 ====\n",
      "1. 處理影片\n",
      "2. 從攝影機即時辨識\n",
      "0. 退出\n",
      "影片資訊：\n",
      "- 解析度：1920x1080\n",
      "- FPS:29\n",
      "- 總幀數：79\n",
      "- 時長：2.72 秒\n",
      "開始處理影片...\n",
      "處理進度：0.0% (0/79 幀)\n",
      "\n",
      "影片處理完成!\n",
      "輸出影片已儲存至：C:\\Users\\User\\OneDrive\\圖片\\相機相簿\\WIN_20250520_21_21_07_Pro.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "class VideoRecognitionSystem:\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化系統\"\"\"\n",
    "        self._init_directories()\n",
    "        self._init_face_models()\n",
    "        self._init_pose_model()\n",
    "        self._init_hand_detector()  \n",
    "        \n",
    "    \"\"\"創建必要的目錄\"\"\"\n",
    "    def _init_directories(self):\n",
    "        self.model_dir = \"models\"\n",
    "        self.temp_dir = \"temp\"\n",
    "        \n",
    "        for directory in [self.model_dir, self.temp_dir]:\n",
    "            Path(directory).mkdir(exist_ok=True)\n",
    "    \n",
    "    \"\"\"初始化手勢檢測器\"\"\"\n",
    "    def _init_hand_detector(self):\n",
    "        \n",
    "        try:\n",
    "            base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "            options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                                  num_hands=4,\n",
    "                                                  min_hand_detection_confidence=0.5,\n",
    "                                                  min_hand_presence_confidence=0.5,\n",
    "                                                  min_tracking_confidence=0.5)\n",
    "            self.hand_detector = vision.HandLandmarker.create_from_options(options)\n",
    "            print(\"MediaPipe 手勢檢測器已成功載入\")\n",
    "        except Exception as e:\n",
    "            print(f\"警告：無法載入 MediaPipe 手勢檢測器：{e}\")\n",
    "            # Fallback to a simple placeholder if loading fails\n",
    "            self.hand_detector = type('', (), {\n",
    "                'detect': lambda x: type('', (), {'hand_landmarks': []})()\n",
    "            })()\n",
    "    \n",
    "    \"\"\"初始化臉部辨識模型\"\"\"\n",
    "    def _init_face_models(self):\n",
    "        # 初始化 dlib 模型\n",
    "        self.face_detector = dlib.get_frontal_face_detector()\n",
    "        \n",
    "        # 檢查模型是否存在\n",
    "        shape_predictor_path = r\"C:\\Users\\User\\project\\shape_predictor_5_face_landmarks.dat\"\n",
    "        if not os.path.exists(shape_predictor_path):\n",
    "            print(f\"警告：找不到 {shape_predictor_path}，部分臉部對齊功能將不可用\")\n",
    "            self.shape_predictor = None\n",
    "        else:\n",
    "            self.shape_predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "            \n",
    "        \n",
    "        face_rec_model_path = r\"C:\\Users\\User\\project\\dlib_face_recognition_resnet_model_v1.dat\"\n",
    "        if not os.path.exists(face_rec_model_path):\n",
    "            print(f\"警告：找不到 {face_rec_model_path}，部分臉部識別功能將不可用\")\n",
    "            self.face_recognizer = None\n",
    "        else:\n",
    "            self.face_recognizer = dlib.face_recognition_model_v1(face_rec_model_path)\n",
    "        \n",
    "        # Opencv2 的人臉檢測器作為備用\n",
    "        self.haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
    "        \n",
    "        # 載入已知人臉編碼\n",
    "        self.known_face_encodings = []\n",
    "        self.known_face_names = []\n",
    "        self.load_face_model()\n",
    "    \n",
    "\n",
    "    \"\"\"初始化 YOLO 動作檢測模型\"\"\"\n",
    "    def _init_pose_model(self):\n",
    "        try:\n",
    "            self.pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "            print(\"YOLO 姿態檢測模型已成功載入\")\n",
    "        except Exception as e:\n",
    "            print(f\"警告：無法載入 YOLO 姿態檢測模型：{e}\")\n",
    "            self.pose_model = None\n",
    "        \n",
    "        # 用於跟蹤手腕位置的字典\n",
    "        self.prev_wrist_x = {}\n",
    "    \n",
    "\n",
    "    \"\"\"載入先前訓練過的臉部辨識模型\"\"\"\n",
    "    def load_face_model(self):\n",
    "        model_path = os.path.join(self.model_dir, \"face_encodings.pickle\")\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "                self.known_face_encodings = data[\"encodings\"]\n",
    "                self.known_face_names = data[\"names\"]\n",
    "            print(f\"臉部辨識模型載入成功，包含 {len(self.known_face_names)} 個人物\")\n",
    "        else:\n",
    "            print(\"沒有找到現有臉部辨識模型，臉部辨識功能將無法識別特定人物\")\n",
    "    \n",
    "    # ================ YOLO動作辨識 ================\n",
    "    \n",
    "    \"\"\"舉手\"\"\"\n",
    "    def is_raising_hand(self, wrist, shoulder):\n",
    "        if np.all(wrist == 0) or np.all(shoulder == 0):\n",
    "            return False\n",
    "        \n",
    "        y_diff = shoulder[1] - wrist[1]\n",
    "        x_diff = abs(wrist[0] - shoulder[0])\n",
    "        return y_diff > 20 and x_diff < 200\n",
    "    \n",
    "    \"\"\"蹲下\"\"\"\n",
    "    def is_squatting(self, hip, knee):\n",
    "        if np.all(hip == 0) or np.all(knee == 0):\n",
    "            return False\n",
    "        return (knee[1] - hip[1]) < 40\n",
    "    \n",
    "    \"\"\"揮手\"\"\"\n",
    "    def is_waving_hand(self, track_id, wrist):\n",
    "        if np.all(wrist == 0):\n",
    "            return False\n",
    "        x_now = wrist[0]\n",
    "        waving = False\n",
    "        if track_id in self.prev_wrist_x:\n",
    "            diff = abs(x_now - self.prev_wrist_x[track_id])\n",
    "            if diff > 20:\n",
    "                waving = True\n",
    "        self.prev_wrist_x[track_id] = x_now\n",
    "        return waving\n",
    "    \n",
    "    \n",
    "\n",
    "    # ================ Mediapipe手勢辨識 ================\n",
    "\n",
    "    \"\"\"檢測OK手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_ok_sign(landmarks):\n",
    "        thumb_tip = np.array([landmarks[4].x, landmarks[4].y])\n",
    "        index_tip = np.array([landmarks[8].x, landmarks[8].y])\n",
    "        distance = np.linalg.norm(thumb_tip - index_tip)\n",
    "        middle_tip = landmarks[12].y\n",
    "        ring_tip = landmarks[16].y\n",
    "        pinky_tip = landmarks[20].y\n",
    "        palm_base = landmarks[0].y\n",
    "        return distance < 0.05 and middle_tip < palm_base and ring_tip < palm_base and pinky_tip < palm_base\n",
    "    \n",
    "    \"\"\"Peace手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_peace_sign(landmarks):\n",
    "        def is_extended(pip, tip):\n",
    "            return tip.y < pip.y\n",
    "        return (\n",
    "            is_extended(landmarks[6], landmarks[8]) and\n",
    "            is_extended(landmarks[10], landmarks[12]) and\n",
    "            landmarks[16].y > landmarks[14].y and\n",
    "            landmarks[20].y > landmarks[18].y\n",
    "        )\n",
    "    \n",
    "    \"\"\"張手手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_open_palm(landmarks):\n",
    "        return all(landmarks[tip].y < landmarks[tip - 2].y for tip in [8, 12, 16, 20])\n",
    "\n",
    "    \"\"\"檢測握拳手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_fist(landmarks):\n",
    "        wrist = np.array([landmarks[0].x, landmarks[0].y])\n",
    "        closed_fingers = [np.linalg.norm(wrist - np.array([landmarks[i].x, landmarks[i].y])) < 0.1 for i in [8, 12, 16, 20]]\n",
    "        return all(closed_fingers)\n",
    "\n",
    "    \"\"\"檢測拍手手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_clapping(hands):\n",
    "        if len(hands) != 2:\n",
    "            return False\n",
    "        p1 = np.array([hands[0][5].x, hands[0][5].y])\n",
    "        p2 = np.array([hands[1][5].x, hands[1][5].y])\n",
    "        return np.linalg.norm(p1 - p2) < 0.08\n",
    "    \n",
    "    # ================繪圖與手勢顯示 ===============\n",
    "    def draw_landmarks_on_image(self, image, detection_result, thickness=3):\n",
    "        annotated_image = image.copy()\n",
    "        hands = detection_result.hand_landmarks\n",
    "        \n",
    "        # 修正：正確檢查是否有手部並檢測拍手\n",
    "        clap_shown = False\n",
    "        if hands and len(hands) >= 2:\n",
    "            clap_shown = self.is_clapping(hands)\n",
    "\n",
    "        for landmarks in hands or []:\n",
    "            gesture = \"\"\n",
    "            # 修正：移除不必要的 self 參數\n",
    "            if self.is_ok_sign(landmarks):\n",
    "                gesture = \"OK Sign\"\n",
    "            elif self.is_peace_sign(landmarks):\n",
    "                gesture = \"Peace Sign\"\n",
    "            elif self.is_open_palm(landmarks):\n",
    "                gesture = \"Open Palm\"\n",
    "            elif self.is_fist(landmarks):\n",
    "                gesture = \"Fist\"\n",
    "            elif clap_shown:\n",
    "                gesture = \"Clap\"\n",
    "\n",
    "            for landmark in landmarks:\n",
    "                x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "                cv2.circle(annotated_image, (x, y), 7, (0, 0, 255), -1)\n",
    "            connections = mp.solutions.hands.HAND_CONNECTIONS\n",
    "\n",
    "            for connection in connections:\n",
    "                start_idx, end_idx = connection\n",
    "                start = landmarks[start_idx]\n",
    "                end = landmarks[end_idx]\n",
    "                start_point = (int(start.x * image.shape[1]), int(start.y * image.shape[0]))\n",
    "                end_point = (int(end.x * image.shape[1]), int(end.y * image.shape[0]))\n",
    "                cv2.line(annotated_image, start_point, end_point, (0, 255, 0), thickness)\n",
    "\n",
    "            if gesture and not clap_shown:\n",
    "                x, y = int(landmarks[0].x * image.shape[1]), int(landmarks[0].y * image.shape[0])\n",
    "                cv2.putText(annotated_image, gesture, (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
    "\n",
    "        if clap_shown:\n",
    "            cv2.putText(annotated_image, \"Clap\", (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 3)\n",
    "\n",
    "        return annotated_image\n",
    "    \n",
    "    # ================ 臉部辨識功能 ================\n",
    "    \n",
    "    \"\"\"在圖像中識別人臉\"\"\"\n",
    "    def recognize_face(self, image, tolerance=0.6):\n",
    "        if not self.known_face_encodings:\n",
    "            # 只檢測人臉位置但不進行識別\n",
    "            face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "            \n",
    "            # 如果找不到人臉，使用 Opencv2 嘗試檢測正面和側面\n",
    "            if not face_locations:\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                # 檢測正面\n",
    "                frontal_faces = self.haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "                for (x, y, w, h) in frontal_faces:\n",
    "                    face_locations.append((y, x+w, y+h, x))\n",
    "            \n",
    "                # 檢測側面\n",
    "                profile_faces = self.profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "                for (x, y, w, h) in profile_faces:\n",
    "                    # 檢查是否與已檢測的面孔重疊\n",
    "                    is_new = True\n",
    "                    for (top, right, bottom, left) in face_locations:\n",
    "                        # 如果中心點落在已有的人臉框內，則不添加\n",
    "                        center_x, center_y = x + w//2, y + h//2\n",
    "                        if left <= center_x <= right and top <= center_y <= bottom:\n",
    "                            is_new = False\n",
    "                            break\n",
    "                    if is_new:\n",
    "                        face_locations.append((y, x+w, y+h, x))\n",
    "            \n",
    "            # 繪製人臉框\n",
    "            for (top, right, bottom, left) in face_locations:\n",
    "                cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                cv2.putText(image, \"人臉\", (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "        else:\n",
    "            # 檢測所有人臉\n",
    "            face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "            \n",
    "            # 如果找不到人臉，使用 Opencv2 嘗試檢測正面和側面\n",
    "            if not face_locations:\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                # 檢測正面\n",
    "                frontal_faces = self.haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "                for (x, y, w, h) in frontal_faces:\n",
    "                    face_locations.append((y, x+w, y+h, x))\n",
    "            \n",
    "                # 檢測側面\n",
    "                profile_faces = self.profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "                for (x, y, w, h) in profile_faces:\n",
    "                    # 檢查是否與已檢測的面孔重疊\n",
    "                    is_new = True\n",
    "                    for (top, right, bottom, left) in face_locations:\n",
    "                        # 如果中心點落在已有的人臉框內，則不添加\n",
    "                        center_x, center_y = x + w//2, y + h//2\n",
    "                        if left <= center_x <= right and top <= center_y <= bottom:\n",
    "                            is_new = False\n",
    "                            break\n",
    "                    if is_new:\n",
    "                        face_locations.append((y, x+w, y+h, x))\n",
    "            \n",
    "            # 如果有檢測到人臉，進行識別\n",
    "            if face_locations:\n",
    "                face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "                \n",
    "                # 辨識每個人臉\n",
    "                for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "                    # 與所有已知人臉比對\n",
    "                    matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding, tolerance=tolerance)\n",
    "                    name = \"未知\"\n",
    "                    \n",
    "                    # 如果有匹配項，使用距離最近的那個\n",
    "                    if True in matches:\n",
    "                        face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "                        best_match_index = np.argmin(face_distances)\n",
    "                        confidence = 1 - face_distances[best_match_index]\n",
    "                        if matches[best_match_index]:\n",
    "                            name = f\"{self.known_face_names[best_match_index]} ({confidence:.2f})\"\n",
    "                    \n",
    "                    # 繪製人臉框和名字\n",
    "                    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                    cv2.rectangle(image, (left, bottom - 35), (right, bottom), (0, 255, 0), cv2.FILLED)\n",
    "                    cv2.putText(image, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"處理單一幀，應用所有識別功能\"\"\"\n",
    "        original_frame = frame.copy()\n",
    "        results = None\n",
    "\n",
    "        # 處理手勢識別\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "        hand_result = self.hand_detector.detect(mp_image)\n",
    "        frame = self.draw_landmarks_on_image(frame, hand_result)\n",
    "        \n",
    "        # 1. 使用 YOLO 進行姿態識別\n",
    "        if self.pose_model:\n",
    "            try:\n",
    "                results = self.pose_model.track(frame, persist=True, tracker=\"botsort.yaml\")\n",
    "                \n",
    "                if results:\n",
    "                    for r in results:\n",
    "                        annotated = r.plot()\n",
    "                        \n",
    "                        if hasattr(r, \"keypoints\") and r.keypoints is not None:\n",
    "                            kps = r.keypoints.xy.numpy()\n",
    "                            if len(kps) > 0:\n",
    "                                for idx, kp in enumerate(kps):\n",
    "                                    track_id = int(r.id[idx]) if hasattr(r, \"id\") and r.id is not None else idx\n",
    "                                    \n",
    "                                    # 提取關鍵點\n",
    "                                    lw = kp[9]; rw = kp[10]\n",
    "                                    lsh = kp[5]; rsh = kp[6]\n",
    "                                    lhip = kp[11]; rhip = kp[12]\n",
    "                                    lknee = kp[13]; rknee = kp[14]\n",
    "                                    \n",
    "                                    # 檢測姿態\n",
    "                                    left_up = self.is_raising_hand(lw, lsh)\n",
    "                                    right_up = self.is_raising_hand(rw, rsh)\n",
    "                                    squat_left = self.is_squatting(lhip, lknee)\n",
    "                                    squat_right = self.is_squatting(rhip, rknee)\n",
    "                                    wave_left = self.is_waving_hand(f\"{track_id}_L\", lw)\n",
    "                                    wave_right = self.is_waving_hand(f\"{track_id}_R\", rw)\n",
    "                                    \n",
    "                                    # 顯示結果\n",
    "                                    text = \"\"\n",
    "                                    if left_up and right_up:\n",
    "                                        text = \"both hands \"\n",
    "                                    elif left_up:\n",
    "                                        text = \"left hand \"\n",
    "                                    elif right_up:\n",
    "                                        text = \"right hand \"\n",
    "                                    \n",
    "                                    if squat_left and squat_right:\n",
    "                                        text += \"knees down \"\n",
    "                                    if wave_left or wave_right:\n",
    "                                        text += \"waving \"\n",
    "                                    \n",
    "                                    if text:\n",
    "                                        x, y = int(kp[0][0]), int(kp[0][1])\n",
    "                                        cv2.putText(annotated, f\"ID {track_id} {text}\", \n",
    "                                                   (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                        \n",
    "                        frame = annotated.copy()\n",
    "            except Exception as e:\n",
    "                print(f\"YOLO 姿態識別錯誤: {e}\")\n",
    "        \n",
    "        # 2. 使用 face_recognition 進行臉部辨識\n",
    "        try:\n",
    "            frame = self.recognize_face(frame)\n",
    "        except Exception as e:\n",
    "            print(f\"臉部辨識錯誤: {e}\")\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def process_video(self, video_path, output_path=None, process_every_n_frames=1):\n",
    "        \"\"\"處理影片\"\"\"\n",
    "        # 開啟影片\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(f\"錯誤：無法開啟影片檔案 {video_path}\")\n",
    "            return None\n",
    "        \n",
    "        # 獲取影片屬性\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        print(f\"影片資訊：\")\n",
    "        print(f\"- 解析度：{width}x{height}\")\n",
    "        print(f\"- FPS:{fps}\")\n",
    "        print(f\"- 總幀數：{total_frames}\")\n",
    "        print(f\"- 時長：{total_frames/fps:.2f} 秒\")\n",
    "        \n",
    "        # 如果需要儲存輸出影片\n",
    "        out = None\n",
    "        if output_path:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        # 處理影片\n",
    "        frame_count = 0\n",
    "        \n",
    "        print(\"開始處理影片...\")\n",
    "        \n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # 顯示進度\n",
    "                if frame_count % 30 == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"處理進度：{progress:.1f}% ({frame_count}/{total_frames} 幀)\")\n",
    "                \n",
    "                # 只處理每第 n 幀以提高效能\n",
    "                if frame_count % process_every_n_frames == 0:\n",
    "                    # 處理幀\n",
    "                    processed_frame = self.process_frame(frame)\n",
    "                    \n",
    "                    # 在畫面左上角顯示幀數和時間\n",
    "                    time_text = f\"幀：{frame_count} | 時間：{frame_count/fps:.2f}s\"\n",
    "                    cv2.putText(processed_frame, time_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                    \n",
    "                    # 如果需要儲存輸出影片\n",
    "                    if out:\n",
    "                        out.write(processed_frame)\n",
    "                    \n",
    "                    # 顯示結果\n",
    "                    cv2.imshow('影片處理', cv2.resize(processed_frame, (900, 680)))\n",
    "                    if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "                        print(\"使用者中斷處理\")\n",
    "                        break\n",
    "                \n",
    "                frame_count += 1\n",
    "                \n",
    "        finally:\n",
    "            # 釋放資源\n",
    "            cap.release()\n",
    "            if out:\n",
    "                out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        print(f\"\\n影片處理完成!\")\n",
    "        if output_path:\n",
    "            print(f\"輸出影片已儲存至：{output_path}\")\n",
    "    \n",
    "    def process_webcam(self, camera_id=0):\n",
    "        \"\"\"從網路攝影機進行即時識別\"\"\"\n",
    "        # 開啟攝影機\n",
    "        cap = cv2.VideoCapture(camera_id)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"錯誤：無法開啟攝影機\")\n",
    "            return\n",
    "        \n",
    "        print(\"開始即時辨識。按空白鍵結束。\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # 讀取一幀\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    print(\"無法從攝影機獲取畫面\")\n",
    "                    break\n",
    "                \n",
    "                # 處理幀\n",
    "                processed_frame = self.process_frame(frame)\n",
    "                \n",
    "                # 顯示結果\n",
    "                cv2.imshow('即時辨識', processed_frame)\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "                    break\n",
    "                \n",
    "        finally:\n",
    "            # 釋放資源\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化系統\n",
    "    system = VideoRecognitionSystem()\n",
    "    \n",
    "    # 顯示功能選單\n",
    "    print(\"\\n==== 影片人臉和姿態辨識系統 ====\")\n",
    "    print(\"1. 處理影片\")\n",
    "    print(\"2. 從攝影機即時辨識\")\n",
    "    print(\"0. 退出\")\n",
    "    \n",
    "    choice = input(\"\\n請選擇功能 (0-2): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        video_path = input(\"請輸入影片路徑: \")\n",
    "        output_path = input(\"請輸入輸出影片路徑 (留空則不儲存): \")\n",
    "        if not output_path:\n",
    "            output_path = None\n",
    "        process_every = int(input(\"每隔幾幀處理一次 (建議: 1-5, 數字越大效能越佳但精度較低): \") or \"1\")\n",
    "        system.process_video(video_path, output_path, process_every)\n",
    "        \n",
    "    elif choice == \"2\":\n",
    "        camera_id = int(input(\"請輸入攝影機 ID (預設 0): \") or \"0\")\n",
    "        system.process_webcam(camera_id)\n",
    "        \n",
    "    else:\n",
    "        print(\"感謝使用！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6667cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告：找不到 models\\shape_predictor_5_face_landmarks.dat，部分臉部對齊功能將不可用\n",
      "警告：找不到 models\\dlib_face_recognition_resnet_model_v1.dat，部分臉部識別功能將不可用\n",
      "臉部辨識模型載入成功，包含 69 個人物\n",
      "YOLO 姿態檢測模型已成功載入\n",
      "MediaPipe 手勢檢測器已成功載入\n",
      "\n",
      "==== 影片人臉和姿態辨識系統 ====\n",
      "1. 處理影片\n",
      "2. 從攝影機即時辨識\n",
      "0. 退出\n",
      "開始即時辨識。按空白鍵結束。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "# Suppress unnecessarily verbose ultralytics logs\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "class VideoRecognitionSystem:\n",
    "    \n",
    "    \"\"\"初始化系統\"\"\"\n",
    "    def __init__(self):\n",
    "        self._init_directories()\n",
    "        self._init_face_models()\n",
    "        self._init_pose_model()\n",
    "        self._init_hand_detector()  \n",
    "        \n",
    "    \"\"\"創建必要的目錄\"\"\"\n",
    "    def _init_directories(self):\n",
    "        self.model_dir = \"models\"\n",
    "        self.temp_dir = \"temp\"\n",
    "        \n",
    "        for directory in [self.model_dir, self.temp_dir]:\n",
    "            Path(directory).mkdir(exist_ok=True)\n",
    "    \n",
    "    \"\"\"初始化手勢檢測器\"\"\"\n",
    "    def _init_hand_detector(self):\n",
    "        try:\n",
    "            # 修復：使用正確的MediaPipe手部檢測器初始化方式\n",
    "            self.mp_hands = mp.solutions.hands\n",
    "            self.hands = self.mp_hands.Hands(\n",
    "                static_image_mode=False,\n",
    "                max_num_hands=2,\n",
    "                min_detection_confidence=0.3,\n",
    "                min_tracking_confidence=0.3\n",
    "            )\n",
    "            self.mp_drawing = mp.solutions.drawing_utils\n",
    "            print(\"MediaPipe 手勢檢測器已成功載入\")\n",
    "        except Exception as e:\n",
    "            print(f\"警告：無法載入 MediaPipe 手勢檢測器：{e}\")\n",
    "            self.hands = None\n",
    "            self.mp_hands = None\n",
    "            self.mp_drawing = None\n",
    "    \n",
    "    \"\"\"初始化臉部辨識模型\"\"\"\n",
    "    def _init_face_models(self):\n",
    "        # 初始化 dlib 模型\n",
    "        self.face_detector = dlib.get_frontal_face_detector()\n",
    "        \n",
    "        # 檢查模型是否存在\n",
    "        shape_predictor_path = os.path.join(self.model_dir, \"shape_predictor_5_face_landmarks.dat\")\n",
    "        if not os.path.exists(shape_predictor_path):\n",
    "            print(f\"警告：找不到 {shape_predictor_path}，部分臉部對齊功能將不可用\")\n",
    "            self.shape_predictor = None\n",
    "        else:\n",
    "            self.shape_predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "            \n",
    "        face_rec_model_path = os.path.join(self.model_dir, \"dlib_face_recognition_resnet_model_v1.dat\")\n",
    "        if not os.path.exists(face_rec_model_path):\n",
    "            print(f\"警告：找不到 {face_rec_model_path}，部分臉部識別功能將不可用\")\n",
    "            self.face_recognizer = None\n",
    "        else:\n",
    "            self.face_recognizer = dlib.face_recognition_model_v1(face_rec_model_path)\n",
    "        \n",
    "        # OpenCV 的人臉檢測器作為備用\n",
    "        self.haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
    "        \n",
    "        # 載入已知人臉編碼\n",
    "        self.known_face_encodings = []\n",
    "        self.known_face_names = []\n",
    "        self.load_face_model()\n",
    "\n",
    "    \"\"\"初始化 YOLO 模型\"\"\"\n",
    "    def _init_pose_model(self):\n",
    "        try:\n",
    "            self.pose_model = YOLO(\"yolov8n-pose.pt\")\n",
    "            print(\"YOLO 姿態檢測模型已成功載入\")\n",
    "        except Exception as e:\n",
    "            print(f\"警告：無法載入 YOLO 姿態檢測模型：{e}\")\n",
    "            self.pose_model = None\n",
    "        \n",
    "        # 用於跟蹤手腕位置的字典 - 分別追蹤左右手\n",
    "        self.prev_left_wrist_x = {}\n",
    "        self.prev_right_wrist_x = {}\n",
    "        self.wrist_position_history = {}  # 用於更穩定的揮手檢測\n",
    "    \n",
    "    \"\"\"載入先前訓練過的臉部辨識模型\"\"\"\n",
    "    def load_face_model(self):\n",
    "        model_path = os.path.join(self.model_dir, \"face_encodings.pickle\")\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "                self.known_face_encodings = data[\"encodings\"]\n",
    "                self.known_face_names = data[\"names\"]\n",
    "            print(f\"臉部辨識模型載入成功，包含 {len(self.known_face_names)} 個人物\")\n",
    "        else:\n",
    "            print(\"沒有找到現有臉部辨識模型，臉部辨識功能將無法識別特定人物\")\n",
    "    \n",
    "    # ================ 動作辨識功能 ================\n",
    "    \n",
    "    \"\"\"舉手檢測\"\"\"\n",
    "    def is_raising_hand(self, wrist, shoulder, elbow=None):\n",
    "        if wrist is None or shoulder is None or len(wrist) < 2 or len(shoulder) < 2:\n",
    "            return False\n",
    "            \n",
    "        if np.all(wrist == 0) or np.all(shoulder == 0):\n",
    "            return False\n",
    "        \n",
    "        # 手腕必須比肩膀高\n",
    "        y_diff = shoulder[1] - wrist[1]\n",
    "        \n",
    "        # 檢查手腕是否在肩膀附近的合理範圍內\n",
    "        x_diff = abs(wrist[0] - shoulder[0])\n",
    "        \n",
    "        # 更寬鬆的條件來檢測舉手\n",
    "        is_above_shoulder = y_diff > 15  # 降低閾值\n",
    "        is_reasonable_distance = x_diff < 300  # 增加允許的橫向距離\n",
    "        \n",
    "        # 如果有肘部信息，加入肘部檢查\n",
    "        if elbow is not None and len(elbow) >= 2 and not np.all(elbow == 0):\n",
    "            # 手腕應該比肘部高（或接近）\n",
    "            elbow_check = (elbow[1] - wrist[1]) >= -20  # 允許手腕稍低於肘部\n",
    "            return is_above_shoulder and is_reasonable_distance and elbow_check\n",
    "        \n",
    "        return is_above_shoulder and is_reasonable_distance\n",
    "    \n",
    "    \"\"\"蹲下檢測\"\"\"\n",
    "    def is_squatting(self, hip, knee):\n",
    "        if hip is None or knee is None or len(hip) < 2 or len(knee) < 2:\n",
    "            return False\n",
    "            \n",
    "        if np.all(hip == 0) or np.all(knee == 0):\n",
    "            return False\n",
    "        return (knee[1] - hip[1]) < 50  # 調整閾值\n",
    "    \n",
    "    \"\"\"揮手檢測\"\"\"\n",
    "    def is_waving_hand(self, track_id, wrist, side='left'):\n",
    "        if wrist is None or len(wrist) < 2 or np.all(wrist == 0):\n",
    "            return False\n",
    "        \n",
    "        x_now = wrist[0]\n",
    "        waving = False\n",
    "        \n",
    "        # 根據左右手選擇對應的歷史記錄\n",
    "        if side == 'left':\n",
    "            prev_dict = self.prev_left_wrist_x\n",
    "        else:\n",
    "            prev_dict = self.prev_right_wrist_x\n",
    "        \n",
    "        # 初始化位置歷史\n",
    "        if track_id not in self.wrist_position_history:\n",
    "            self.wrist_position_history[track_id] = {'left': [], 'right': []}\n",
    "        \n",
    "        # 記錄位置歷史（保留最近5個位置）\n",
    "        history = self.wrist_position_history[track_id][side]\n",
    "        history.append(x_now)\n",
    "        if len(history) > 5:\n",
    "            history.pop(0)\n",
    "        \n",
    "        # 檢查是否有足夠的歷史記錄來判斷揮手\n",
    "        if len(history) >= 3:\n",
    "            # 計算位置變化\n",
    "            recent_changes = [abs(history[i] - history[i-1]) for i in range(1, len(history))]\n",
    "            avg_change = sum(recent_changes) / len(recent_changes)\n",
    "            \n",
    "            # 如果平均變化超過閾值，認為是揮手\n",
    "            if avg_change > 15:\n",
    "                waving = True\n",
    "        \n",
    "        # 保持原有的簡單檢測作為備用\n",
    "        if track_id in prev_dict:\n",
    "            diff = abs(x_now - prev_dict[track_id])\n",
    "            if diff > 25:  # 調整閾值\n",
    "                waving = True\n",
    "        \n",
    "        prev_dict[track_id] = x_now\n",
    "        return waving\n",
    "    \n",
    "    \"\"\"ok手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_ok_sign(landmarks):\n",
    "        if not landmarks or len(landmarks) < 21:\n",
    "            return False\n",
    "        try:\n",
    "            thumb_tip = np.array([landmarks[4].x, landmarks[4].y])\n",
    "            index_tip = np.array([landmarks[8].x, landmarks[8].y])\n",
    "            distance = np.linalg.norm(thumb_tip - index_tip)\n",
    "            middle_tip = landmarks[12].y\n",
    "            ring_tip = landmarks[16].y\n",
    "            pinky_tip = landmarks[20].y\n",
    "            palm_base = landmarks[0].y\n",
    "            return distance < 0.05 and middle_tip < palm_base and ring_tip < palm_base and pinky_tip < palm_base\n",
    "        except (IndexError, AttributeError):\n",
    "            return False\n",
    "    \n",
    "    \"\"\"peace手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_peace_sign(landmarks):\n",
    "        if not landmarks or len(landmarks) < 21:\n",
    "            return False\n",
    "        try:\n",
    "            def is_extended(pip, tip):\n",
    "                return tip.y < pip.y\n",
    "            return (\n",
    "                is_extended(landmarks[6], landmarks[8]) and\n",
    "                is_extended(landmarks[10], landmarks[12]) and\n",
    "                landmarks[16].y > landmarks[14].y and\n",
    "                landmarks[20].y > landmarks[18].y\n",
    "            )\n",
    "        except (IndexError, AttributeError):\n",
    "            return False\n",
    "    \n",
    "    \"\"\"open_palm手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_open_palm(landmarks):\n",
    "        if not landmarks or len(landmarks) < 21:\n",
    "            return False\n",
    "        try:\n",
    "            return all(landmarks[tip].y < landmarks[tip - 2].y for tip in [8, 12, 16, 20])\n",
    "        except (IndexError, AttributeError):\n",
    "            return False\n",
    "\n",
    "    \"\"\"握拳手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_fist(landmarks):\n",
    "        if not landmarks or len(landmarks) < 21:\n",
    "            return False\n",
    "        try:\n",
    "            wrist = np.array([landmarks[0].x, landmarks[0].y])\n",
    "            closed_fingers = [np.linalg.norm(wrist - np.array([landmarks[i].x, landmarks[i].y])) < 0.1 for i in [8, 12, 16, 20]]\n",
    "            return all(closed_fingers)\n",
    "        except (IndexError, AttributeError):\n",
    "            return False\n",
    "\n",
    "    \"\"\"clap手勢\"\"\"\n",
    "    @staticmethod\n",
    "    def is_clapping(hands):\n",
    "        if not hands or len(hands) != 2:\n",
    "            return False\n",
    "        try:\n",
    "            p1 = np.array([hands[0][5].x, hands[0][5].y])\n",
    "            p2 = np.array([hands[1][5].x, hands[1][5].y])\n",
    "            return np.linalg.norm(p1 - p2) < 0.08\n",
    "        except (IndexError, AttributeError):\n",
    "            return False\n",
    "    \n",
    "    # ================修復的繪圖與手勢顯示 ===============\n",
    "\n",
    "    \"\"\"繪製手部標記並識別手勢\"\"\"\n",
    "    def draw_landmarks_on_image(self, image, results):\n",
    "        if image is None or self.hands is None:\n",
    "            return image\n",
    "        \n",
    "        annotated_image = image.copy()\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            clap_shown = False\n",
    "            if len(results.multi_hand_landmarks) >= 2:\n",
    "                clap_shown = self.is_clapping(results.multi_hand_landmarks)\n",
    "\n",
    "            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                gesture = \"\"\n",
    "                hand_label = \"\"\n",
    "                \n",
    "                # 獲取手部標籤（左手或右手）\n",
    "                if results.multi_handedness and idx < len(results.multi_handedness):\n",
    "                    hand_label = results.multi_handedness[idx].classification[0].label\n",
    "                \n",
    "                # 檢測手勢\n",
    "                if self.is_ok_sign(hand_landmarks.landmark):\n",
    "                    gesture = f\"OK Sign\"\n",
    "                elif self.is_peace_sign(hand_landmarks.landmark):\n",
    "                    gesture = f\"Peace Sign\"\n",
    "                elif self.is_open_palm(hand_landmarks.landmark):\n",
    "                    gesture = f\"Open Palm\"\n",
    "                elif self.is_fist(hand_landmarks.landmark):\n",
    "                    gesture = f\"Fist\"\n",
    "                elif clap_shown:\n",
    "                    gesture = \"Clap\"\n",
    "\n",
    "                try:\n",
    "                    # 繪製手部標記\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        annotated_image, hand_landmarks, self.mp_hands.HAND_CONNECTIONS,\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),\n",
    "                        self.mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2)\n",
    "                    )\n",
    "\n",
    "                    # 顯示手勢名稱和手部標籤\n",
    "                    if gesture and not clap_shown:\n",
    "                        x = int(hand_landmarks.landmark[0].x * image.shape[1])\n",
    "                        y = int(hand_landmarks.landmark[0].y * image.shape[0])\n",
    "                        cv2.putText(annotated_image, gesture, (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "                    elif hand_label:\n",
    "                        x = int(hand_landmarks.landmark[0].x * image.shape[1])\n",
    "                        y = int(hand_landmarks.landmark[0].y * image.shape[0])\n",
    "                        cv2.putText(annotated_image, f\"{hand_label} Hand\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                        \n",
    "                except (IndexError, AttributeError, ValueError) as e:\n",
    "                    print(f\"繪製手部標記時發生錯誤: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # 如果檢測到拍手，顯示在畫面頂部\n",
    "            if clap_shown:\n",
    "                cv2.putText(annotated_image, \"Clap\", (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 3)\n",
    "\n",
    "        return annotated_image\n",
    "    \n",
    "    # ================ 臉部辨識功能 ================\n",
    "    \"\"\"圖像中識別人臉\"\"\"\n",
    "    def recognize_face(self, image, tolerance=0.6):\n",
    "        if image is None or image.size == 0:\n",
    "            return image\n",
    "        \n",
    "        output_image = image.copy()\n",
    "        \n",
    "        try:\n",
    "            face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "            \n",
    "            if not face_locations:\n",
    "                if len(image.shape) == 2:\n",
    "                    gray = image\n",
    "                else:\n",
    "                    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                frontal_faces = self.haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "                for (x, y, w, h) in frontal_faces:\n",
    "                    face_locations.append((y, x+w, y+h, x))\n",
    "            \n",
    "                profile_faces = self.profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "                for (x, y, w, h) in profile_faces:\n",
    "                    is_new = True\n",
    "                    for (top, right, bottom, left) in face_locations:\n",
    "                        center_x, center_y = x + w//2, y + h//2\n",
    "                        if left <= center_x <= right and top <= center_y <= bottom:\n",
    "                            is_new = False\n",
    "                            break\n",
    "                    if is_new:\n",
    "                        face_locations.append((y, x+w, y+h, x))\n",
    "            \n",
    "            if not self.known_face_encodings:\n",
    "                for (top, right, bottom, left) in face_locations:\n",
    "                    cv2.rectangle(output_image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                    cv2.putText(output_image, \"人臉\", (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "            else:\n",
    "                if face_locations:\n",
    "                    face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "                    \n",
    "                    for idx, ((top, right, bottom, left), face_encoding) in enumerate(zip(face_locations, face_encodings)):\n",
    "                        matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding, tolerance=tolerance)\n",
    "                        name = \"未知\"\n",
    "                        \n",
    "                        if True in matches:\n",
    "                            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "                            best_match_index = np.argmin(face_distances)\n",
    "                            confidence = 1 - face_distances[best_match_index]\n",
    "                            if matches[best_match_index]:\n",
    "                                name = f\"{self.known_face_names[best_match_index]} ({confidence:.2f})\"\n",
    "                        \n",
    "                        cv2.rectangle(output_image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                        cv2.rectangle(output_image, (left, bottom - 35), (right, bottom), (0, 255, 0), cv2.FILLED)\n",
    "                        cv2.putText(output_image, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "        except Exception as e:\n",
    "            print(f\"臉部辨識錯誤: {e}\")\n",
    "            return image\n",
    "        \n",
    "        return output_image\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"處理單一幀，應用所有識別功能\"\"\"\n",
    "        if frame is None or frame.size == 0:\n",
    "            print(\"收到空幀，跳過處理\")\n",
    "            return np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        try:\n",
    "            processed_frame = frame.copy()\n",
    "            \n",
    "            # 1. 處理手勢識別\n",
    "            if self.hands is not None:\n",
    "                try:\n",
    "                    rgb_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                    hand_results = self.hands.process(rgb_frame)\n",
    "                    processed_frame = self.draw_landmarks_on_image(processed_frame, hand_results)\n",
    "                except Exception as e:\n",
    "                    print(f\"手勢識別錯誤: {e}\")\n",
    "            \n",
    "            # 2. 使用 YOLO 進行姿態識別\n",
    "            if self.pose_model:\n",
    "                try:\n",
    "                    results = self.pose_model.track(processed_frame, persist=True, tracker=\"botsort.yaml\", verbose=False)\n",
    "                    \n",
    "                    if results and len(results) > 0:\n",
    "                        for r in results:\n",
    "                            try:\n",
    "                                annotated = r.plot()\n",
    "                                \n",
    "                                if hasattr(r, \"keypoints\") and r.keypoints is not None and r.keypoints.xy is not None:\n",
    "                                    kps = r.keypoints.xy.cpu().numpy() if hasattr(r.keypoints.xy, 'cpu') else r.keypoints.xy.numpy()\n",
    "                                    \n",
    "                                    if len(kps) > 0:\n",
    "                                        for idx, kp in enumerate(kps):\n",
    "                                            # 安全地獲取 track_id\n",
    "                                            track_id = idx  # 預設值\n",
    "                                            if hasattr(r, \"id\") and r.id is not None:\n",
    "                                                try:\n",
    "                                                    ids = r.id.cpu().numpy() if hasattr(r.id, 'cpu') else r.id.numpy()\n",
    "                                                    if idx < len(ids):\n",
    "                                                        track_id = int(ids[idx])\n",
    "                                                except:\n",
    "                                                    track_id = idx\n",
    "                                            \n",
    "                                            # YOLO Pose 關鍵點索引檢查\n",
    "                                            if len(kp) >= 17:  # 確保有足夠的關鍵點\n",
    "                                                # 提取關鍵點並檢查有效性\n",
    "                                                def get_keypoint(kp, index):\n",
    "                                                    if index < len(kp) and len(kp[index]) >= 2:\n",
    "                                                        return kp[index]\n",
    "                                                    return np.array([0, 0])\n",
    "                                                \n",
    "                                                left_wrist = get_keypoint(kp, 9)      # 左手腕\n",
    "                                                right_wrist = get_keypoint(kp, 10)    # 右手腕\n",
    "                                                left_shoulder = get_keypoint(kp, 5)   # 左肩\n",
    "                                                right_shoulder = get_keypoint(kp, 6)  # 右肩\n",
    "                                                left_elbow = get_keypoint(kp, 7)      # 左肘\n",
    "                                                right_elbow = get_keypoint(kp, 8)     # 右肘\n",
    "                                                left_hip = get_keypoint(kp, 11)       # 左髖\n",
    "                                                right_hip = get_keypoint(kp, 12)      # 右髖\n",
    "                                                left_knee = get_keypoint(kp, 13)      # 左膝\n",
    "                                                right_knee = get_keypoint(kp, 14)     # 右膝\n",
    "                                                \n",
    "                                                # 檢測姿態\n",
    "                                                left_hand_up = self.is_raising_hand(left_wrist, left_shoulder, left_elbow)\n",
    "                                                right_hand_up = self.is_raising_hand(right_wrist, right_shoulder, right_elbow)\n",
    "                                                squat_left = self.is_squatting(left_hip, left_knee)\n",
    "                                                squat_right = self.is_squatting(right_hip, right_knee)\n",
    "                                                wave_left = self.is_waving_hand(track_id, left_wrist, 'left')\n",
    "                                                wave_right = self.is_waving_hand(track_id, right_wrist, 'right')\n",
    "                                                \n",
    "                                                # 顯示結果\n",
    "                                                actions = []\n",
    "                                                if left_hand_up and right_hand_up:\n",
    "                                                    actions.append(\"both hands up\")\n",
    "                                                elif left_hand_up:\n",
    "                                                    actions.append(\"left hand up\")\n",
    "                                                elif right_hand_up:\n",
    "                                                    actions.append(\"right hand up\")\n",
    "                                                \n",
    "                                                if squat_left and squat_right:\n",
    "                                                    actions.append(\"squatting\")\n",
    "                                                \n",
    "                                                if wave_left:\n",
    "                                                    actions.append(\"waving left\")\n",
    "                                                if wave_right:\n",
    "                                                    actions.append(\"waving right\")\n",
    "                                                \n",
    "                                                # 在畫面上顯示動作\n",
    "                                                if actions:\n",
    "                                                    # 找一個合適的位置顯示文字\n",
    "                                                    text_x, text_y = 50, 50 + idx * 30  # 預設位置，每個人物錯開\n",
    "                                                    \n",
    "                                                    if len(kp) > 0 and len(kp[0]) >= 2 and not np.all(kp[0] == 0):\n",
    "                                                        text_x, text_y = int(kp[0][0]), int(kp[0][1]) - 30\n",
    "                                                    elif not np.all(left_shoulder == 0):\n",
    "                                                        text_x, text_y = int(left_shoulder[0]), int(left_shoulder[1]) - 30\n",
    "                                                    elif not np.all(right_shoulder == 0):\n",
    "                                                        text_x, text_y = int(right_shoulder[0]), int(right_shoulder[1]) - 30\n",
    "                                                    \n",
    "                                                    action_text = f\"ID {track_id}: {', '.join(actions)}\"\n",
    "                                                    cv2.putText(annotated, action_text, \n",
    "                                                              (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                                \n",
    "                                processed_frame = annotated.copy()\n",
    "                            except Exception as e:\n",
    "                                print(f\"YOLO 結果處理錯誤: {str(e)}\")\n",
    "                                continue\n",
    "                except Exception as e:\n",
    "                    print(f\"YOLO 姿態識別錯誤: {str(e)}\")\n",
    "            \n",
    "            # 3. 使用 face_recognition 進行臉部辨識\n",
    "            try:\n",
    "                processed_frame = self.recognize_face(processed_frame)\n",
    "            except Exception as e:\n",
    "                print(f\"臉部辨識錯誤: {str(e)}\")\n",
    "            \n",
    "            return processed_frame\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"幀處理錯誤: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            if frame is not None and frame.size > 0:\n",
    "                return frame\n",
    "            else:\n",
    "                return np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    \n",
    "    def process_video(self, video_path, output_path=None, process_every_n_frames=1):\n",
    "        \"\"\"處理影片\"\"\"\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"錯誤：影片檔案不存在 - {video_path}\")\n",
    "            return None\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(f\"錯誤：無法開啟影片檔案 {video_path}\")\n",
    "            return None\n",
    "        \n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if fps <= 0:\n",
    "            print(\"警告：無法獲取正確的 FPS，使用預設值 30\")\n",
    "            fps = 30\n",
    "        if width <= 0 or height <= 0:\n",
    "            print(\"警告：無法獲取正確的解析度，使用預設值 640x480\")\n",
    "            width, height = 640, 480\n",
    "        if total_frames <= 0:\n",
    "            print(\"警告：無法獲取正確的總幀數\")\n",
    "            total_frames = 1000\n",
    "        \n",
    "        print(f\"影片資訊：\")\n",
    "        print(f\"- 解析度：{width}x{height}\")\n",
    "        print(f\"- FPS:{fps}\")\n",
    "        print(f\"- 總幀數：{total_frames}\")\n",
    "        print(f\"- 時長：{total_frames/fps:.2f} 秒\")\n",
    "        \n",
    "        out = None\n",
    "        if output_path:\n",
    "            output_dir = r'C:\\Users\\User\\project\\a'\n",
    "    \n",
    "        if not os.path.exists(output_dir):\n",
    "             os.makedirs(output_dir)\n",
    "\n",
    "        # 加上檔案名稱\n",
    "        output_path = os.path.join(output_dir, \"result.mp4\")\n",
    "    \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "        if not out.isOpened():\n",
    "            print(f\"錯誤：無法建立輸出影片檔案 {output_path}\")\n",
    "            out = None\n",
    "\n",
    "        frame_count = 0\n",
    "\n",
    "        \n",
    "        print(\"開始處理影片...\")\n",
    "        \n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # 顯示進度\n",
    "                if frame_count % 30 == 0:\n",
    "                    progress = (frame_count / total_frames) * 100\n",
    "                    print(f\"處理進度：{progress:.1f}% ({frame_count}/{total_frames} 幀)\")\n",
    "                \n",
    "                # 只處理每第 n 幀以提高效能\n",
    "                if frame_count % process_every_n_frames == 0:\n",
    "                    # 處理幀\n",
    "                    processed_frame = self.process_frame(frame)\n",
    "                    \n",
    "                    # 在畫面左上角顯示幀數和時間\n",
    "                    time_text = f\"幀：{frame_count} | 時間：{frame_count/fps:.2f}s\"\n",
    "                    cv2.putText(processed_frame, time_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                    \n",
    "                    # 如果需要儲存輸出影片\n",
    "                    if out:\n",
    "                        out.write(processed_frame)\n",
    "                    \n",
    "                    # 顯示結果\n",
    "                    cv2.imshow('影片處理', cv2.resize(processed_frame, (900, 680)))\n",
    "                    if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "                        print(\"使用者中斷處理\")\n",
    "                        break\n",
    "                \n",
    "                frame_count += 1\n",
    "                \n",
    "        finally:\n",
    "            # 釋放資源\n",
    "            cap.release()\n",
    "            if out:\n",
    "                out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        print(f\"\\n影片處理完成!\")\n",
    "        if output_path:\n",
    "            print(f\"輸出影片已儲存至：{output_path}\")\n",
    "    \n",
    "    def process_webcam(self, camera_id=0):\n",
    "        \"\"\"從網路攝影機進行即時識別\"\"\"\n",
    "        # 開啟攝影機\n",
    "        cap = cv2.VideoCapture(camera_id)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"錯誤：無法開啟攝影機\")\n",
    "            return\n",
    "        \n",
    "        print(\"開始即時辨識。按空白鍵結束。\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # 讀取一幀\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    print(\"無法從攝影機獲取畫面\")\n",
    "                    break\n",
    "                \n",
    "                # 處理幀\n",
    "                processed_frame = self.process_frame(frame)\n",
    "                \n",
    "                # 顯示結果\n",
    "                cv2.imshow('即時辨識', processed_frame)\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "                    break\n",
    "                \n",
    "        finally:\n",
    "            # 釋放資源\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化系統\n",
    "    system = VideoRecognitionSystem()\n",
    "    \n",
    "    # 顯示功能選單\n",
    "    print(\"\\n==== 影片人臉和姿態辨識系統 ====\")\n",
    "    print(\"1. 處理影片\")\n",
    "    print(\"2. 從攝影機即時辨識\")\n",
    "    print(\"0. 退出\")\n",
    "    \n",
    "    choice = input(\"\\n請選擇功能 (0-2): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        video_path = input(\"請輸入影片路徑: \")\n",
    "        output_path = input(\"請輸入輸出影片路徑 (留空則不儲存): \")\n",
    "        if not output_path:\n",
    "            output_path = None\n",
    "        process_every = int(input(\"每隔幾幀處理一次 (建議: 1-5, 數字越大效能越佳但精度較低): \") or \"1\")\n",
    "        system.process_video(video_path, output_path, process_every)\n",
    "        \n",
    "    elif choice == \"2\":\n",
    "        camera_id = int(input(\"請輸入攝影機 ID (預設 0): \") or \"0\")\n",
    "        system.process_webcam(camera_id)\n",
    "        \n",
    "    else:\n",
    "        print(\"感謝使用！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "412770538",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
