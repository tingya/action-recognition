{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7609aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q ultralytics\n",
    "#!pip install opencv-python\n",
    "#!pip install numpy\n",
    "#!pip install mediapipe\n",
    "#!pip install --upgrade mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edaafc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hand_landmarker (2).task'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "wget.download('https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e19855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https://ultralytics.com/images/bus.jpg to 'bus.jpg'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134k/134k [00:00<00:00, 741kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 c:\\Users\\User\\yolo_yo\\predict\\bus.jpg: 640x480 4 persons, 172.9ms\n",
      "Speed: 10.6ms preprocess, 172.9ms inference, 13.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9910, 0.9289, 0.9869, 0.4267, 0.9313, 0.9907, 0.9976, 0.9248, 0.9884, 0.9015, 0.9744, 0.9969, 0.9984, 0.9949, 0.9975, 0.9785, 0.9856],\n",
      "        [0.1586, 0.1561, 0.0468, 0.2351, 0.0505, 0.6711, 0.2402, 0.5964, 0.1104, 0.4541, 0.1319, 0.7288, 0.5135, 0.7590, 0.5564, 0.5935, 0.4370],\n",
      "        [0.9894, 0.9335, 0.9794, 0.5549, 0.9086, 0.9952, 0.9976, 0.9465, 0.9778, 0.9134, 0.9481, 0.9983, 0.9987, 0.9954, 0.9964, 0.9774, 0.9804],\n",
      "        [0.0987, 0.0392, 0.0631, 0.0392, 0.0677, 0.2103, 0.2339, 0.2615, 0.3053, 0.3423, 0.3554, 0.2780, 0.2918, 0.2393, 0.2481, 0.1393, 0.1388]])\n",
      "data: tensor([[[1.4236e+02, 4.4186e+02, 9.9095e-01],\n",
      "         [1.4799e+02, 4.3142e+02, 9.2890e-01],\n",
      "         [1.3054e+02, 4.3337e+02, 9.8691e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2672e-01],\n",
      "         [1.0718e+02, 4.4066e+02, 9.3134e-01],\n",
      "         [1.5745e+02, 4.9311e+02, 9.9074e-01],\n",
      "         [9.4264e+01, 4.9925e+02, 9.9755e-01],\n",
      "         [1.7646e+02, 5.5098e+02, 9.2482e-01],\n",
      "         [1.1066e+02, 5.6757e+02, 9.8838e-01],\n",
      "         [1.7422e+02, 5.3235e+02, 9.0151e-01],\n",
      "         [1.6200e+02, 5.3439e+02, 9.7442e-01],\n",
      "         [1.4883e+02, 6.4514e+02, 9.9693e-01],\n",
      "         [9.9666e+01, 6.4941e+02, 9.9845e-01],\n",
      "         [1.7887e+02, 7.4929e+02, 9.9495e-01],\n",
      "         [9.4800e+01, 7.5651e+02, 9.9747e-01],\n",
      "         [1.8598e+02, 8.4975e+02, 9.7854e-01],\n",
      "         [7.3946e+01, 8.5814e+02, 9.8563e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 1.5855e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5608e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.6807e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3511e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 5.0521e-02],\n",
      "         [8.0997e+02, 4.8721e+02, 6.7113e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4024e-01],\n",
      "         [7.8853e+02, 5.6880e+02, 5.9636e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1045e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.5411e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3185e-01],\n",
      "         [8.0153e+02, 6.4043e+02, 7.2879e-01],\n",
      "         [8.0040e+02, 6.3644e+02, 5.1345e-01],\n",
      "         [7.6838e+02, 7.3066e+02, 7.5898e-01],\n",
      "         [7.7332e+02, 7.2789e+02, 5.5644e-01],\n",
      "         [7.2683e+02, 8.4367e+02, 5.9346e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.3701e-01]],\n",
      "\n",
      "        [[2.9259e+02, 4.5085e+02, 9.8936e-01],\n",
      "         [2.9837e+02, 4.4170e+02, 9.3348e-01],\n",
      "         [2.8318e+02, 4.4337e+02, 9.7940e-01],\n",
      "         [3.0378e+02, 4.4555e+02, 5.5487e-01],\n",
      "         [2.6531e+02, 4.4892e+02, 9.0857e-01],\n",
      "         [3.1815e+02, 4.9541e+02, 9.9517e-01],\n",
      "         [2.5092e+02, 4.9919e+02, 9.9764e-01],\n",
      "         [3.2882e+02, 5.5388e+02, 9.4647e-01],\n",
      "         [2.5059e+02, 5.7485e+02, 9.7776e-01],\n",
      "         [2.7947e+02, 5.3499e+02, 9.1344e-01],\n",
      "         [2.5247e+02, 6.1571e+02, 9.4807e-01],\n",
      "         [3.0349e+02, 6.2994e+02, 9.9828e-01],\n",
      "         [2.5931e+02, 6.2951e+02, 9.9869e-01],\n",
      "         [3.0109e+02, 7.2364e+02, 9.9536e-01],\n",
      "         [2.6114e+02, 7.1774e+02, 9.9643e-01],\n",
      "         [2.8824e+02, 8.0623e+02, 9.7735e-01],\n",
      "         [2.6060e+02, 8.0521e+02, 9.8038e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 9.8715e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9226e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 6.3093e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 3.9153e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 6.7749e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.1034e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3390e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.6152e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0528e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4233e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5543e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7804e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.9177e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.3934e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4810e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3926e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3879e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (1080, 810)\n",
      "shape: torch.Size([4, 17, 3])\n",
      "xy: tensor([[[142.3645, 441.8609],\n",
      "         [147.9898, 431.4181],\n",
      "         [130.5442, 433.3710],\n",
      "         [  0.0000,   0.0000],\n",
      "         [107.1835, 440.6562],\n",
      "         [157.4519, 493.1140],\n",
      "         [ 94.2637, 499.2470],\n",
      "         [176.4613, 550.9834],\n",
      "         [110.6624, 567.5748],\n",
      "         [174.2182, 532.3488],\n",
      "         [162.0031, 534.3919],\n",
      "         [148.8314, 645.1359],\n",
      "         [ 99.6655, 649.4141],\n",
      "         [178.8723, 749.2934],\n",
      "         [ 94.7997, 756.5117],\n",
      "         [185.9789, 849.7502],\n",
      "         [ 73.9457, 858.1396]],\n",
      "\n",
      "        [[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [809.9744, 487.2143],\n",
      "         [  0.0000,   0.0000],\n",
      "         [788.5313, 568.7997],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [801.5266, 640.4258],\n",
      "         [800.4036, 636.4432],\n",
      "         [768.3763, 730.6614],\n",
      "         [773.3247, 727.8928],\n",
      "         [726.8314, 843.6720],\n",
      "         [  0.0000,   0.0000]],\n",
      "\n",
      "        [[292.5922, 450.8507],\n",
      "         [298.3656, 441.7009],\n",
      "         [283.1804, 443.3694],\n",
      "         [303.7812, 445.5459],\n",
      "         [265.3112, 448.9188],\n",
      "         [318.1479, 495.4087],\n",
      "         [250.9237, 499.1886],\n",
      "         [328.8164, 553.8760],\n",
      "         [250.5860, 574.8477],\n",
      "         [279.4675, 534.9937],\n",
      "         [252.4710, 615.7097],\n",
      "         [303.4948, 629.9416],\n",
      "         [259.3111, 629.5056],\n",
      "         [301.0946, 723.6394],\n",
      "         [261.1398, 717.7366],\n",
      "         [288.2359, 806.2347],\n",
      "         [260.5974, 805.2137]],\n",
      "\n",
      "        [[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000]]])\n",
      "xyn: tensor([[[0.1758, 0.4091],\n",
      "         [0.1827, 0.3995],\n",
      "         [0.1612, 0.4013],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1323, 0.4080],\n",
      "         [0.1944, 0.4566],\n",
      "         [0.1164, 0.4623],\n",
      "         [0.2179, 0.5102],\n",
      "         [0.1366, 0.5255],\n",
      "         [0.2151, 0.4929],\n",
      "         [0.2000, 0.4948],\n",
      "         [0.1837, 0.5973],\n",
      "         [0.1230, 0.6013],\n",
      "         [0.2208, 0.6938],\n",
      "         [0.1170, 0.7005],\n",
      "         [0.2296, 0.7868],\n",
      "         [0.0913, 0.7946]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [1.0000, 0.4511],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.9735, 0.5267],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.9895, 0.5930],\n",
      "         [0.9882, 0.5893],\n",
      "         [0.9486, 0.6765],\n",
      "         [0.9547, 0.6740],\n",
      "         [0.8973, 0.7812],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3612, 0.4175],\n",
      "         [0.3684, 0.4090],\n",
      "         [0.3496, 0.4105],\n",
      "         [0.3750, 0.4125],\n",
      "         [0.3275, 0.4157],\n",
      "         [0.3928, 0.4587],\n",
      "         [0.3098, 0.4622],\n",
      "         [0.4059, 0.5128],\n",
      "         [0.3094, 0.5323],\n",
      "         [0.3450, 0.4954],\n",
      "         [0.3117, 0.5701],\n",
      "         [0.3747, 0.5833],\n",
      "         [0.3201, 0.5829],\n",
      "         [0.3717, 0.6700],\n",
      "         [0.3224, 0.6646],\n",
      "         [0.3558, 0.7465],\n",
      "         [0.3217, 0.7456]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load a model\n",
    "# yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11l.pt, yolo11x.pt 分別對應到 YOLO tiny, small, medium, large, extra-large model\n",
    "# yolo11n.pt, yolo11n-seg, yolo11n-pose, yolo11n-cls 分別進行 image localization, image segmentation, image keypoint detection, image classification\n",
    "# model.track 進行物體追蹤\n",
    "#\n",
    "model = YOLO(\"yolo11n-pose.pt\")  # load an official model\n",
    "# model = YOLO(\"path/to/best.pt\")  # load a custom model\n",
    "\n",
    "# Predict with the model\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
    "#results = model.track(0, show=True)\n",
    "\n",
    "for r in results:\n",
    "    r.save()\n",
    "    print(r.keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd74fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影片播放完畢\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import logging\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "model = YOLO(\"yolo11n-pose.pt\")\n",
    "\n",
    "# MediaPipe hand detector\n",
    "model_path = r\"C:\\Users\\User\\yolo_yo\\predict\\hand_landmarker.task\"\n",
    "base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "video_path = \"WIN_20250520_21_42_55_Pro.mp4\"\n",
    "cap = cv.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"無法開啟影片\")\n",
    "    exit()\n",
    "\n",
    "prev_wrist_x = {}\n",
    "\n",
    "# ---------------- 手勢判斷 ---------------- #\n",
    "def is_raising_hand(wrist, shoulder):\n",
    "    if np.all(wrist == 0) or np.all(shoulder == 0):\n",
    "        return False\n",
    "    \n",
    "    y_diff = shoulder[1] - wrist[1]\n",
    "    x_diff = abs(wrist[0] - shoulder[0])\n",
    "    return y_diff > 20 and x_diff < 200\n",
    "\n",
    "def is_squatting(hip, knee):\n",
    "    if np.all(hip == 0) or np.all(knee == 0):\n",
    "        return False\n",
    "    return (knee[1] - hip[1]) < 40\n",
    "\n",
    "def is_waving_hand(track_id, wrist):\n",
    "    if np.all(wrist == 0):\n",
    "        return False\n",
    "    x_now = wrist[0]\n",
    "    waving = False\n",
    "    if track_id in prev_wrist_x:\n",
    "        diff = abs(x_now - prev_wrist_x[track_id])\n",
    "        if diff > 20:\n",
    "            waving = True\n",
    "    prev_wrist_x[track_id] = x_now\n",
    "    return waving\n",
    "\n",
    "def is_ok_sign(landmarks):\n",
    "    thumb_tip = np.array([landmarks[4].x, landmarks[4].y])\n",
    "    index_tip = np.array([landmarks[8].x, landmarks[8].y])\n",
    "    distance = np.linalg.norm(thumb_tip - index_tip)\n",
    "    middle_tip = landmarks[12].y\n",
    "    ring_tip = landmarks[16].y\n",
    "    pinky_tip = landmarks[20].y\n",
    "    palm_base = landmarks[0].y\n",
    "    return distance < 0.05 and middle_tip < palm_base and ring_tip < palm_base and pinky_tip < palm_base\n",
    "\n",
    "def is_peace_sign(landmarks):\n",
    "    def is_extended(pip, tip):\n",
    "        return tip.y < pip.y\n",
    "    return (\n",
    "        is_extended(landmarks[6], landmarks[8]) and\n",
    "        is_extended(landmarks[10], landmarks[12]) and\n",
    "        landmarks[16].y > landmarks[14].y and\n",
    "        landmarks[20].y > landmarks[18].y\n",
    "    )\n",
    "\n",
    "def is_open_palm(landmarks):\n",
    "    return all(landmarks[tip].y < landmarks[tip - 2].y for tip in [8, 12, 16, 20])\n",
    "\n",
    "def is_fist(landmarks):\n",
    "    wrist = np.array([landmarks[0].x, landmarks[0].y])\n",
    "    closed_fingers = [np.linalg.norm(wrist - np.array([landmarks[i].x, landmarks[i].y])) < 0.1 for i in [8, 12, 16, 20]]\n",
    "    return all(closed_fingers)\n",
    "\n",
    "def is_clapping(hands):\n",
    "    if len(hands) != 2:\n",
    "        return False\n",
    "    p1 = np.array([hands[0][5].x, hands[0][5].y])\n",
    "    p2 = np.array([hands[1][5].x, hands[1][5].y])\n",
    "    return np.linalg.norm(p1 - p2) < 0.08\n",
    "\n",
    "# ---------------- 繪圖與手勢顯示 ---------------- #\n",
    "def draw_landmarks_on_image(image, detection_result, thickness=3):\n",
    "    annotated_image = image.copy()\n",
    "    hands = detection_result.hand_landmarks\n",
    "    clap_shown = is_clapping(hands) if hands else False\n",
    "\n",
    "    for landmarks in hands or []:\n",
    "        gesture = \"\"\n",
    "        if is_ok_sign(landmarks):\n",
    "            gesture = \"OK Sign\"\n",
    "        elif is_peace_sign(landmarks):\n",
    "            gesture = \"Peace Sign\"\n",
    "        elif is_open_palm(landmarks):\n",
    "            gesture = \"Open Palm\"\n",
    "        elif is_fist(landmarks):\n",
    "            gesture = \"Fist\"\n",
    "        elif clap_shown:\n",
    "            gesture = \"Clap\"\n",
    "\n",
    "        for landmark in landmarks:\n",
    "            x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "            cv.circle(annotated_image, (x, y), 7, (0, 0, 255), -1)\n",
    "        connections = mp.solutions.hands.HAND_CONNECTIONS\n",
    "        for connection in connections:\n",
    "            start_idx, end_idx = connection\n",
    "            start = landmarks[start_idx]\n",
    "            end = landmarks[end_idx]\n",
    "            start_point = (int(start.x * image.shape[1]), int(start.y * image.shape[0]))\n",
    "            end_point = (int(end.x * image.shape[1]), int(end.y * image.shape[0]))\n",
    "            cv.line(annotated_image, start_point, end_point, (0, 255, 0), thickness)\n",
    "\n",
    "        if gesture and not clap_shown:\n",
    "            x, y = int(landmarks[0].x * image.shape[1]), int(landmarks[0].y * image.shape[0])\n",
    "            cv.putText(annotated_image, gesture, (x, y - 20), cv.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
    "\n",
    "    if clap_shown:\n",
    "        cv.putText(annotated_image, \"Clap\", (50, 80), cv.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 3)\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "# ---------------- 主迴圈 ---------------- #\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"影片播放完畢\")\n",
    "        break\n",
    "\n",
    "    results = model.track(frame, persist=True, tracker=\"botsort.yaml\")\n",
    "\n",
    "    if results:\n",
    "        for r in results:\n",
    "            annotated = r.plot()\n",
    "            if hasattr(r, \"keypoints\") and r.keypoints is not None:\n",
    "                kps = r.keypoints.xy.numpy()\n",
    "                if len(kps) > 0:\n",
    "                    for idx, kp in enumerate(kps):\n",
    "                        track_id = int(r.id[idx]) if hasattr(r, \"id\") and r.id is not None else idx\n",
    "\n",
    "                        lw = kp[9]; rw = kp[10]\n",
    "                        lsh = kp[5]; rsh = kp[6]\n",
    "                        lhip = kp[11]; rhip = kp[12]\n",
    "                        lknee = kp[13]; rknee = kp[14]\n",
    "\n",
    "                        left_up = is_raising_hand(lw, lsh)\n",
    "                        right_up = is_raising_hand(rw, rsh)\n",
    "                        squat_left = is_squatting(lhip, lknee)\n",
    "                        squat_right = is_squatting(rhip, rknee)\n",
    "\n",
    "                        frame_rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "                        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "                        detection_result = detector.detect(mp_image)\n",
    "                        is_clap = is_clapping(detection_result.hand_landmarks if detection_result else [])\n",
    "\n",
    "                        wave_left = is_waving_hand(f\"{track_id}_L\", lw) if not is_clap else False\n",
    "                        wave_right = is_waving_hand(f\"{track_id}_R\", rw) if not is_clap else False\n",
    "\n",
    "                        text = \"\"\n",
    "                        if left_up and right_up:\n",
    "                            text = \"both hands \"\n",
    "                        elif left_up:\n",
    "                            text = \"left hand \"\n",
    "                        elif right_up:\n",
    "                            text = \"right hand \"\n",
    "\n",
    "                        if squat_left and squat_right:\n",
    "                            text += \"knees down \"\n",
    "                        if wave_left or wave_right:\n",
    "                            text += \"waving \"\n",
    "\n",
    "                        if text:\n",
    "                            x, y = int(kp[0][0]), int(kp[0][1])\n",
    "                            cv.putText(annotated, f\"ID {track_id} {text}\", (x, y - 10),\n",
    "                                       cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "                        annotated_frame = draw_landmarks_on_image(annotated, detection_result)\n",
    "                        cv.imshow(\"YOLO + MediaPipe Pose\", cv.resize(annotated_frame, (900, 680)))\n",
    "\n",
    "    if cv.waitKey(10) & 0xFF == ord(' '):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "412770538",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
